{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af75f9a1",
   "metadata": {},
   "source": [
    "# ALS Attemp\n",
    "This notebook will contain the attempt of doing ALS on the DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afc7eb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/08 19:56:27 WARN Utils: Your hostname, Batman, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/07/08 19:56:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/08 19:56:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.255.255.254:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ALS_Recommender</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f745a37b2c0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing the required pyspark library\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "#Setup Spark Session\n",
    "spark = SparkSession.builder.appName('ALS_Recommender').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22f238f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|     17|   4.0|944249077|\n",
      "|     1|     25|   1.0|944250228|\n",
      "|     1|     29|   2.0|943230976|\n",
      "|     1|     30|   5.0|944249077|\n",
      "|     1|     32|   5.0|943228858|\n",
      "+------+-------+------+---------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "#CSV file can be downloaded from the link mentioned above.\n",
    "data = spark.read.csv('../data/raw/ratings.csv',\n",
    "                      inferSchema=True,header=True)\n",
    "\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e5f7590",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/08 20:02:19 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+--------------------+\n",
      "|summary|            userId|           movieId|            rating|           timestamp|\n",
      "+-------+------------------+------------------+------------------+--------------------+\n",
      "|  count|          32000204|          32000204|          32000204|            32000204|\n",
      "|   mean|100278.50641102163|29318.610121829224|3.5403956487277393|1.2752411995732634E9|\n",
      "| stddev| 57949.04623325339| 50958.16087967063|1.0589862139453108| 2.561629759505963E8|\n",
      "|    min|                 1|                 1|               0.5|           789652004|\n",
      "|    max|            200948|            292757|               5.0|          1697164147|\n",
      "+-------+------------------+------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "247ba84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing the data using random split into train_data and test_data \n",
    "# in 80% and 20% respectively\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ac679c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/08 20:06:00 WARN BlockManager: Block rdd_39_7 could not be removed as it was not found on disk or in memory\n",
      "25/07/08 20:06:02 WARN BlockManager: Block rdd_40_7 could not be removed as it was not found on disk or in memory\n",
      "25/07/08 20:06:02 WARN BlockManager: Block rdd_39_4 could not be removed as it was not found on disk or in memory\n",
      "25/07/08 20:06:02 WARN BlockManager: Block rdd_40_4 could not be removed as it was not found on disk or in memory\n",
      "25/07/08 20:06:04 WARN BlockManager: Block rdd_39_8 could not be removed as it was not found on disk or in memory\n",
      "25/07/08 20:06:04 WARN BlockManager: Block rdd_40_8 could not be removed as it was not found on disk or in memory\n",
      "25/07/08 20:06:03 WARN BlockManager: Block rdd_39_1 could not be removed as it was not found on disk or in memory\n",
      "25/07/08 20:06:03 ERROR Executor: Exception in task 8.0 in stage 9.0 (TID 76)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.Integer.valueOf(Integer.java:1073)\n",
      "\tat scala.runtime.BoxesRunTime.boxToInteger(BoxesRunTime.java:63)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.add(ALS.scala:1460)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$6(ALS.scala:1674)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda/0x00007fee64f8d810.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat org.apache.spark.util.collection.CompactBuffer.foreach(CompactBuffer.scala:32)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1673)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda/0x00007fee64f8ca28.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:754)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda/0x00007fee651745a0.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:234)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007fee64dd57a0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00007fee651cfbc8.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getOrElseUpdate$1(BlockManager.scala:1407)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007fee651b9af0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007fee64dd57a0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "25/07/08 20:06:04 ERROR Executor: Exception in task 7.0 in stage 9.0 (TID 75)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.mkArray(ArrayBuilder.scala:290)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.resize(ArrayBuilder.scala:296)\n",
      "\tat scala.collection.mutable.ArrayBuilder.ensureSize(ArrayBuilder.scala:39)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.addOne(ArrayBuilder.scala:301)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.addOne(ArrayBuilder.scala:285)\n",
      "\tat scala.collection.mutable.Growable.$plus$eq(Growable.scala:36)\n",
      "\tat scala.collection.mutable.Growable.$plus$eq$(Growable.scala:36)\n",
      "\tat scala.collection.mutable.ArrayBuilder.$plus$eq(ArrayBuilder.scala:24)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.add(ALS.scala:1460)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$6(ALS.scala:1674)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda/0x00007fee64f8d810.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat org.apache.spark.util.collection.CompactBuffer.foreach(CompactBuffer.scala:32)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1673)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda/0x00007fee64f8ca28.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:754)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda/0x00007fee651745a0.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:234)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007fee64dd57a0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:388)\n",
      "25/07/08 20:06:04 ERROR Executor: Exception in task 4.0 in stage 9.0 (TID 72)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.mkArray(ArrayBuilder.scala:290)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.resize(ArrayBuilder.scala:296)\n",
      "\tat scala.collection.mutable.ArrayBuilder.ensureSize(ArrayBuilder.scala:39)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.addOne(ArrayBuilder.scala:301)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.addOne(ArrayBuilder.scala:285)\n",
      "\tat scala.collection.mutable.Growable.$plus$eq(Growable.scala:36)\n",
      "\tat scala.collection.mutable.Growable.$plus$eq$(Growable.scala:36)\n",
      "\tat scala.collection.mutable.ArrayBuilder.$plus$eq(ArrayBuilder.scala:24)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.add(ALS.scala:1460)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$6(ALS.scala:1674)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda/0x00007fee64f8d810.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat org.apache.spark.util.collection.CompactBuffer.foreach(CompactBuffer.scala:32)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1673)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda/0x00007fee64f8ca28.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:754)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda/0x00007fee651745a0.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:234)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007fee64dd57a0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:388)\n",
      "25/07/08 20:06:03 WARN BlockManager: Block rdd_40_1 could not be removed as it was not found on disk or in memory\n",
      "25/07/08 20:06:03 ERROR Executor: Exception in task 1.0 in stage 9.0 (TID 69)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.mkArray(ArrayBuilder.scala:374)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.resize(ArrayBuilder.scala:380)\n",
      "\tat scala.collection.mutable.ArrayBuilder.ensureSize(ArrayBuilder.scala:39)\n",
      "\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:71)\n",
      "\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:24)\n",
      "\tat scala.collection.mutable.Growable.$plus$plus$eq(Growable.scala:69)\n",
      "\tat scala.collection.mutable.Growable.$plus$plus$eq$(Growable.scala:69)\n",
      "\tat scala.collection.mutable.ArrayBuilder.$plus$plus$eq(ArrayBuilder.scala:24)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.add(ALS.scala:1457)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$6(ALS.scala:1674)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda/0x00007fee64f8d810.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat org.apache.spark.util.collection.CompactBuffer.foreach(CompactBuffer.scala:32)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1673)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda/0x00007fee64f8ca28.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:754)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda/0x00007fee651745a0.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:234)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007fee64dd57a0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:388)\n",
      "25/07/08 20:06:04 WARN BlockManager: Block rdd_39_9 could not be removed as it was not found on disk or in memory\n",
      "25/07/08 20:06:05 WARN BlockManager: Block rdd_40_9 could not be removed as it was not found on disk or in memory\n",
      "25/07/08 20:06:05 ERROR Executor: Exception in task 9.0 in stage 9.0 (TID 77)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/07/08 20:06:05 WARN BlockManager: Block rdd_39_5 could not be removed as it was not found on disk or in memory\n",
      "25/07/08 20:06:05 WARN BlockManager: Block rdd_40_5 could not be removed as it was not found on disk or in memory\n",
      "25/07/08 20:06:05 ERROR Executor: Exception in task 5.0 in stage 9.0 (TID 73)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/07/08 20:06:05 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[#178,Executor task launch worker for task 9.0 in stage 9.0 (TID 77),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/07/08 20:06:05 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[#183,Executor task launch worker for task 4.0 in stage 9.0 (TID 72),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.mkArray(ArrayBuilder.scala:290)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.resize(ArrayBuilder.scala:296)\n",
      "\tat scala.collection.mutable.ArrayBuilder.ensureSize(ArrayBuilder.scala:39)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.addOne(ArrayBuilder.scala:301)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.addOne(ArrayBuilder.scala:285)\n",
      "\tat scala.collection.mutable.Growable.$plus$eq(Growable.scala:36)\n",
      "\tat scala.collection.mutable.Growable.$plus$eq$(Growable.scala:36)\n",
      "\tat scala.collection.mutable.ArrayBuilder.$plus$eq(ArrayBuilder.scala:24)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.add(ALS.scala:1460)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$6(ALS.scala:1674)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda/0x00007fee64f8d810.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat org.apache.spark.util.collection.CompactBuffer.foreach(CompactBuffer.scala:32)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1673)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda/0x00007fee64f8ca28.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:754)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda/0x00007fee651745a0.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:234)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007fee64dd57a0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:388)\n",
      "25/07/08 20:06:05 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[#189,Executor task launch worker for task 8.0 in stage 9.0 (TID 76),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.Integer.valueOf(Integer.java:1073)\n",
      "\tat scala.runtime.BoxesRunTime.boxToInteger(BoxesRunTime.java:63)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.add(ALS.scala:1460)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$6(ALS.scala:1674)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda/0x00007fee64f8d810.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat org.apache.spark.util.collection.CompactBuffer.foreach(CompactBuffer.scala:32)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1673)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda/0x00007fee64f8ca28.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:754)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda/0x00007fee651745a0.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:234)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007fee64dd57a0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x00007fee651cfbc8.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getOrElseUpdate$1(BlockManager.scala:1407)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007fee651b9af0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007fee64dd57a0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "25/07/08 20:06:05 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[#177,Executor task launch worker for task 7.0 in stage 9.0 (TID 75),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.mkArray(ArrayBuilder.scala:290)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.resize(ArrayBuilder.scala:296)\n",
      "\tat scala.collection.mutable.ArrayBuilder.ensureSize(ArrayBuilder.scala:39)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.addOne(ArrayBuilder.scala:301)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofInt.addOne(ArrayBuilder.scala:285)\n",
      "\tat scala.collection.mutable.Growable.$plus$eq(Growable.scala:36)\n",
      "\tat scala.collection.mutable.Growable.$plus$eq$(Growable.scala:36)\n",
      "\tat scala.collection.mutable.ArrayBuilder.$plus$eq(ArrayBuilder.scala:24)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.add(ALS.scala:1460)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$6(ALS.scala:1674)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda/0x00007fee64f8d810.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat org.apache.spark.util.collection.CompactBuffer.foreach(CompactBuffer.scala:32)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1673)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda/0x00007fee64f8ca28.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:754)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda/0x00007fee651745a0.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:234)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007fee64dd57a0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:388)\n",
      "25/07/08 20:06:05 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[#170,Executor task launch worker for task 1.0 in stage 9.0 (TID 69),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.mkArray(ArrayBuilder.scala:374)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.resize(ArrayBuilder.scala:380)\n",
      "\tat scala.collection.mutable.ArrayBuilder.ensureSize(ArrayBuilder.scala:39)\n",
      "\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:71)\n",
      "\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:24)\n",
      "\tat scala.collection.mutable.Growable.$plus$plus$eq(Growable.scala:69)\n",
      "\tat scala.collection.mutable.Growable.$plus$plus$eq$(Growable.scala:69)\n",
      "\tat scala.collection.mutable.ArrayBuilder.$plus$plus$eq(ArrayBuilder.scala:24)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.add(ALS.scala:1457)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$6(ALS.scala:1674)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda/0x00007fee64f8d810.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat org.apache.spark.util.collection.CompactBuffer.foreach(CompactBuffer.scala:32)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1673)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda/0x00007fee64f8ca28.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:754)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda/0x00007fee651745a0.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:234)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007fee64dd57a0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:388)\n",
      "25/07/08 20:06:05 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[#191,Executor task launch worker for task 5.0 in stage 9.0 (TID 73),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/07/08 20:06:05 WARN TaskSetManager: Lost task 1.0 in stage 9.0 (TID 69) (10.255.255.254 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.mkArray(ArrayBuilder.scala:374)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.resize(ArrayBuilder.scala:380)\n",
      "\tat scala.collection.mutable.ArrayBuilder.ensureSize(ArrayBuilder.scala:39)\n",
      "\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:71)\n",
      "\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:24)\n",
      "\tat scala.collection.mutable.Growable.$plus$plus$eq(Growable.scala:69)\n",
      "\tat scala.collection.mutable.Growable.$plus$plus$eq$(Growable.scala:69)\n",
      "\tat scala.collection.mutable.ArrayBuilder.$plus$plus$eq(ArrayBuilder.scala:24)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.add(ALS.scala:1457)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$6(ALS.scala:1674)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda/0x00007fee64f8d810.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat org.apache.spark.util.collection.CompactBuffer.foreach(CompactBuffer.scala:32)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1673)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda/0x00007fee64f8ca28.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:754)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda/0x00007fee651745a0.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:234)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007fee64dd57a0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:388)\n",
      "\n",
      "25/07/08 20:06:05 ERROR TaskSetManager: Task 1 in stage 9.0 failed 1 times; aborting job\n",
      "25/07/08 20:06:05 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 9.0 failed 1 times, most recent failure: Lost task 1.0 in stage 9.0 (TID 69) (10.255.255.254 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.mkArray(ArrayBuilder.scala:374)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.resize(ArrayBuilder.scala:380)\n",
      "\tat scala.collection.mutable.ArrayBuilder.ensureSize(ArrayBuilder.scala:39)\n",
      "\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:71)\n",
      "\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:24)\n",
      "\tat scala.collection.mutable.Growable.$plus$plus$eq(Growable.scala:69)\n",
      "\tat scala.collection.mutable.Growable.$plus$plus$eq$(Growable.scala:69)\n",
      "\tat scala.collection.mutable.ArrayBuilder.$plus$plus$eq(ArrayBuilder.scala:24)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.add(ALS.scala:1457)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$6(ALS.scala:1674)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda/0x00007fee64f8d810.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat org.apache.spark.util.collection.CompactBuffer.foreach(CompactBuffer.scala:32)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1673)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda/0x00007fee64f8ca28.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:754)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda/0x00007fee651745a0.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:234)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007fee64dd57a0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:388)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\n",
      "\tat org.apache.spark.rdd.RDD.count(RDD.scala:1304)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:1004)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.$anonfun$fit$1(ALS.scala:753)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:226)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:226)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:730)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:632)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.mkArray(ArrayBuilder.scala:374)\n",
      "\tat scala.collection.mutable.ArrayBuilder$ofFloat.resize(ArrayBuilder.scala:380)\n",
      "\tat scala.collection.mutable.ArrayBuilder.ensureSize(ArrayBuilder.scala:39)\n",
      "\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:71)\n",
      "\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:24)\n",
      "\tat scala.collection.mutable.Growable.$plus$plus$eq(Growable.scala:69)\n",
      "\tat scala.collection.mutable.Growable.$plus$plus$eq$(Growable.scala:69)\n",
      "\tat scala.collection.mutable.ArrayBuilder.$plus$plus$eq(ArrayBuilder.scala:24)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.add(ALS.scala:1457)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$6(ALS.scala:1674)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda/0x00007fee64f8d810.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat org.apache.spark.util.collection.CompactBuffer.foreach(CompactBuffer.scala:32)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$.$anonfun$makeBlocks$5(ALS.scala:1673)\n",
      "\tat org.apache.spark.ml.recommendation.ALS$$$Lambda/0x00007fee64f8ca28.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:754)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$Lambda/0x00007fee651745a0.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$9.next(Iterator.scala:584)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:234)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:319)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1634)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda/0x00007fee64dd57a0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1560)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1625)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1424)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1378)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:386)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:388)\n",
      "\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m#Fitting the model on the train_data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m model = \u001b[43mals\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/sparkle/lib/python3.12/site-packages/pyspark/ml/base.py:203\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/sparkle/lib/python3.12/site-packages/pyspark/ml/util.py:164\u001b[39m, in \u001b[36mtry_remote_fit.<locals>.wrapped\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/sparkle/lib/python3.12/site-packages/pyspark/ml/wrapper.py:411\u001b[39m, in \u001b[36mJavaEstimator._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    409\u001b[39m \u001b[38;5;129m@try_remote_fit\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) -> JM:\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m     java_model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    412\u001b[39m     model = \u001b[38;5;28mself\u001b[39m._create_model(java_model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/sparkle/lib/python3.12/site-packages/pyspark/ml/wrapper.py:407\u001b[39m, in \u001b[36mJavaEstimator._fit_java\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    406\u001b[39m \u001b[38;5;28mself\u001b[39m._transfer_params_to_java()\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_java_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/sparkle/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/sparkle/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/sparkle/lib/python3.12/site-packages/py4j/protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31m<class 'str'>\u001b[39m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/sparkle/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2205\u001b[39m, in \u001b[36mInteractiveShell.showtraceback\u001b[39m\u001b[34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[39m\n\u001b[32m   2202\u001b[39m         traceback.print_exc()\n\u001b[32m   2203\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2205\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2206\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.call_pdb:\n\u001b[32m   2207\u001b[39m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[32m   2208\u001b[39m     \u001b[38;5;28mself\u001b[39m.debugger(force=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/sparkle/lib/python3.12/site-packages/ipykernel/zmqshell.py:559\u001b[39m, in \u001b[36mZMQInteractiveShell._showtraceback\u001b[39m\u001b[34m(self, etype, evalue, stb)\u001b[39m\n\u001b[32m    553\u001b[39m sys.stdout.flush()\n\u001b[32m    554\u001b[39m sys.stderr.flush()\n\u001b[32m    556\u001b[39m exc_content = {\n\u001b[32m    557\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtraceback\u001b[39m\u001b[33m\"\u001b[39m: stb,\n\u001b[32m    558\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mename\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype.\u001b[34m__name__\u001b[39m),\n\u001b[32m--> \u001b[39m\u001b[32m559\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mevalue\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    560\u001b[39m }\n\u001b[32m    562\u001b[39m dh = \u001b[38;5;28mself\u001b[39m.displayhook\n\u001b[32m    563\u001b[39m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/sparkle/lib/python3.12/site-packages/py4j/protocol.py:472\u001b[39m, in \u001b[36mPy4JJavaError.__str__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    471\u001b[39m     gateway_client = \u001b[38;5;28mself\u001b[39m.java_exception._gateway_client\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m     answer = \u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    473\u001b[39m     return_value = get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    474\u001b[39m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[32m    475\u001b[39m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[32m    476\u001b[39m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/sparkle/lib/python3.12/site-packages/py4j/java_gateway.py:1036\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry=\u001b[38;5;28;01mTrue\u001b[39;00m, binary=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1016\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[33;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1034\u001b[39m \u001b[33;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1038\u001b[39m         response = connection.send_command(command)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/sparkle/lib/python3.12/site-packages/py4j/clientserver.py:284\u001b[39m, in \u001b[36mJavaClient._get_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection.socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/sparkle/lib/python3.12/site-packages/py4j/clientserver.py:291\u001b[39m, in \u001b[36mJavaClient._create_new_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    288\u001b[39m     connection = ClientServerConnection(\n\u001b[32m    289\u001b[39m         \u001b[38;5;28mself\u001b[39m.java_parameters, \u001b[38;5;28mself\u001b[39m.python_parameters,\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mself\u001b[39m.gateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_thread_connection(connection)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/sparkle/lib/python3.12/site-packages/py4j/clientserver.py:438\u001b[39m, in \u001b[36mClientServerConnection.connect_to_java_server\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ssl_context:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.socket = \u001b[38;5;28mself\u001b[39m.ssl_context.wrap_socket(\n\u001b[32m    437\u001b[39m         \u001b[38;5;28mself\u001b[39m.socket, server_hostname=\u001b[38;5;28mself\u001b[39m.java_address)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28mself\u001b[39m.socket.makefile(\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.is_connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [Errno 111] Connection refused"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/sparkle/lib/python3.12/site-packages/IPython/core/async_helpers.py:128\u001b[39m, in \u001b[36m_pseudo_sync_runner\u001b[39m\u001b[34m(coro)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[33;03mA runner that does not really allow async execution, and just advance the coroutine.\u001b[39;00m\n\u001b[32m    122\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    125\u001b[39m \u001b[33;03mCredit to Nathaniel Smith\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc.value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/sparkle/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3413\u001b[39m, in \u001b[36mInteractiveShell.run_cell_async\u001b[39m\u001b[34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple, cell_id)\u001b[39m\n\u001b[32m   3409\u001b[39m exec_count = \u001b[38;5;28mself\u001b[39m.execution_count\n\u001b[32m   3410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.error_in_exec:\n\u001b[32m   3411\u001b[39m     \u001b[38;5;66;03m# Store formatted traceback and error details\u001b[39;00m\n\u001b[32m   3412\u001b[39m     \u001b[38;5;28mself\u001b[39m.history_manager.exceptions[exec_count] = (\n\u001b[32m-> \u001b[39m\u001b[32m3413\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_format_exception_for_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43merror_in_exec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3414\u001b[39m     )\n\u001b[32m   3416\u001b[39m \u001b[38;5;66;03m# Each cell is a *single* input, regardless of how many lines it has\u001b[39;00m\n\u001b[32m   3417\u001b[39m \u001b[38;5;28mself\u001b[39m.execution_count += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/sparkle/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3474\u001b[39m, in \u001b[36mInteractiveShell._format_exception_for_storage\u001b[39m\u001b[34m(self, exception, filename, running_compiled_code)\u001b[39m\n\u001b[32m   3470\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   3471\u001b[39m             \u001b[38;5;66;03m# In case formatting fails, fallback to Python's built-in formatting.\u001b[39;00m\n\u001b[32m   3472\u001b[39m             stb = traceback.format_exception(etype, evalue, tb)\n\u001b[32m-> \u001b[39m\u001b[32m3474\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mename\u001b[39m\u001b[33m\"\u001b[39m: etype.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mevalue\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mtraceback\u001b[39m\u001b[33m\"\u001b[39m: stb}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/sparkle/lib/python3.12/site-packages/py4j/protocol.py:472\u001b[39m, in \u001b[36mPy4JJavaError.__str__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    471\u001b[39m     gateway_client = \u001b[38;5;28mself\u001b[39m.java_exception._gateway_client\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m     answer = \u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    473\u001b[39m     return_value = get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    474\u001b[39m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[32m    475\u001b[39m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[32m    476\u001b[39m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/sparkle/lib/python3.12/site-packages/py4j/java_gateway.py:1036\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry=\u001b[38;5;28;01mTrue\u001b[39;00m, binary=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1016\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[33;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1034\u001b[39m \u001b[33;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1038\u001b[39m         response = connection.send_command(command)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/sparkle/lib/python3.12/site-packages/py4j/clientserver.py:284\u001b[39m, in \u001b[36mJavaClient._get_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection.socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/sparkle/lib/python3.12/site-packages/py4j/clientserver.py:291\u001b[39m, in \u001b[36mJavaClient._create_new_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    288\u001b[39m     connection = ClientServerConnection(\n\u001b[32m    289\u001b[39m         \u001b[38;5;28mself\u001b[39m.java_parameters, \u001b[38;5;28mself\u001b[39m.python_parameters,\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mself\u001b[39m.gateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_thread_connection(connection)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.9/envs/sparkle/lib/python3.12/site-packages/py4j/clientserver.py:438\u001b[39m, in \u001b[36mClientServerConnection.connect_to_java_server\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ssl_context:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.socket = \u001b[38;5;28mself\u001b[39m.ssl_context.wrap_socket(\n\u001b[32m    437\u001b[39m         \u001b[38;5;28mself\u001b[39m.socket, server_hostname=\u001b[38;5;28mself\u001b[39m.java_address)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28mself\u001b[39m.socket.makefile(\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.is_connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "# Build the recommendation model using ALS on the training data\n",
    "als = ALS(maxIter=5,\n",
    "          regParam=0.01,\n",
    "          userCol=\"userId\",\n",
    "          itemCol=\"movieId\",\n",
    "          ratingCol=\"rating\")\n",
    "\n",
    "#Fitting the model on the train_data\n",
    "model = als.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a3730e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparkle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

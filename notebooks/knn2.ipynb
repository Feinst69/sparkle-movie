{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d056b12d",
   "metadata": {},
   "source": [
    "# ATTEMPT TO DO KNN for recommendation.\n",
    "It is actually different from cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73fbbe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler, Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63c54314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/18 09:25:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/07/18 09:25:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://vscode-python-86863-0.vscode-python-86863.user-fenitra.svc.cluster.local:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>KNN_Recommender</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x749238a9ee40>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"KNN_Recommender\") \\\n",
    "    .config(\"spark.driver.memory\", \"100g\") \\\n",
    "    .config(\"spark.executor.memory\", \"100g\") \\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2deca6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 1. Lire les ratings\n",
    "ratings = spark.read.csv(\"../data/raw/ratings.csv\", header=True, inferSchema=True)\n",
    "ratings = ratings.select(\"userId\", \"movieId\", \"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "641a3304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "top_movies = (\n",
    "    ratings.groupBy(\"movieId\")\n",
    "    .agg(count(\"userId\").alias(\"num_ratings\"))\n",
    "    .orderBy(col(\"num_ratings\").desc())\n",
    "    .limit(50000)\n",
    "    .select(\"movieId\")\n",
    "    .rdd.flatMap(lambda x: x)\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beb5d6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.pivotMaxValues\", 85000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bdcb2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/18 09:31:36 WARN DAGScheduler: Broadcasting large task binary with size 1442.5 KiB\n",
      "25/07/18 09:49:37 WARN DAGScheduler: Broadcasting large task binary with size 1449.9 KiB\n",
      "25/07/18 09:49:37 WARN DAGScheduler: Broadcasting large task binary with size 1446.7 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ratings_small = ratings.filter(col(\"movieId\").isin(top_movies))\n",
    "user_item = ratings_small.groupBy(\"userId\").pivot(\"movieId\").avg(\"rating\").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e702e1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = user_item.columns[1:]  # sauf userId\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "user_vectors = assembler.transform(user_item).select(\"userId\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa9e6f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation (pour cosinus)\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"norm_features\", p=2.0)\n",
    "user_vectors = normalizer.transform(user_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f3ac9e",
   "metadata": {},
   "source": [
    "## Calculer la similarité cosinus entre tous les utilisateurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69808eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# On va faire un produit cartésien limité (sur un petit échantillon, exemple 100 users)\n",
    "user_sample = user_vectors.limit(100)  # attention à la taille !\n",
    "user_self = user_sample.alias(\"a\")\n",
    "user_other = user_sample.alias(\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3efb8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produit cartésien, on ne garde pas les paires (i,i)\n",
    "user_pairs = user_self.join(user_other, F.col(\"a.userId\") != F.col(\"b.userId\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4431b98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du produit scalaire (cosinus)\n",
    "from pyspark.ml.linalg import Vectors, DenseVector, VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf18f5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(u, v):\n",
    "    return float(u.dot(v))  # les vecteurs sont normalisés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98c351c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b115b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_udf = F.udf(cos_sim, FloatType())\n",
    "\n",
    "user_pairs = user_pairs.withColumn(\n",
    "    \"similarity\",\n",
    "    cos_sim_udf(F.col(\"a.norm_features\"), F.col(\"b.norm_features\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b434a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour chaque user, garder les k plus proches voisins\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e05ad87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c078c0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/18 10:53:40 WARN DAGScheduler: Broadcasting large task binary with size 1452.0 KiB\n",
      "25/07/18 10:54:10 WARN DAGScheduler: Broadcasting large task binary with size 1452.0 KiB\n",
      "25/07/18 11:15:24 WARN DAGScheduler: Broadcasting large task binary with size 13.1 MiB\n",
      "[Stage 17:>                                                       (0 + 16) / 17]\r"
     ]
    }
   ],
   "source": [
    "k = 50\n",
    "window = Window.partitionBy(\"a.userId\").orderBy(F.desc(\"similarity\"))\n",
    "knn = user_pairs.withColumn(\"rank\", F.row_number().over(window)).filter(F.col(\"rank\") <= k)\n",
    "\n",
    "# Affiche les voisins les plus proches de chaque user\n",
    "knn.select(\n",
    "    F.col(\"a.userId\").alias(\"userId\"),\n",
    "    F.col(\"b.userId\").alias(\"neighborId\"),\n",
    "    \"similarity\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ada1c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 169\n",
    "\n",
    "# Id des k voisins les plus proches\n",
    "neighbors = knn.filter(F.col(\"userId\") == user_id).select(\"neighborId\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Films déjà vus\n",
    "movies_seen = ratings.filter(col(\"userId\") == user_id).select(\"movieId\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Films bien notés par les voisins, mais pas vus\n",
    "neighbor_ratings = ratings.filter(col(\"userId\").isin(neighbors))\n",
    "recos = (\n",
    "    neighbor_ratings\n",
    "    .filter(~col(\"movieId\").isin(movies_seen))\n",
    "    .groupBy(\"movieId\")\n",
    "    .agg(F.avg(\"rating\").alias(\"mean_rating\"))\n",
    "    .orderBy(F.desc(\"mean_rating\"))\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "# Affichage des titres des films\n",
    "movies = spark.read.csv(\"movies.csv\", header=True, inferSchema=True)\n",
    "recos = recos.join(movies, on=\"movieId\", how=\"left\")\n",
    "recos.select(\"movieId\", \"title\", \"mean_rating\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d054457b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

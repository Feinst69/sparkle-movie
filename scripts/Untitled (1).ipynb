{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27c5f17d-2457-4a8c-827b-4bdb400a5f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-4.0.0.tar.gz (434.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.1/434.1 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.9 (from pyspark)\n",
      "  Downloading py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Downloading py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "\u001b[33m  DEPRECATION: Building 'pyspark' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'pyspark'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-4.0.0-py2.py3-none-any.whl size=434741299 sha256=57e1ff8c7bbe30c6737de6098493b916e5956677391c9ae80b3cb8e80eaeeb7e\n",
      "  Stored in directory: /home/onyxia/.cache/pip/wheels/e4/82/ed/8c205a7ade6132d277fcdaccff39051342fff763b34e90dc8f\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [pyspark]m1/2\u001b[0m [pyspark]\n",
      "\u001b[1A\u001b[2KSuccessfully installed py4j-0.10.9.9 pyspark-4.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01dc8a82-d8da-42c1-bad3-118a476d62e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/15 12:39:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MovieLensRecommendation\") \\\n",
    "    .config(\"spark.driver.memory\", \"50g\") \\\n",
    "    .config(\"spark.executor.memory\", \"50g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de38b480-ef85-4374-8332-d959f8d879cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "ratings = spark.read.csv(\"ml-32m/ratings.csv\", header=True, inferSchema=True)\n",
    "movies = spark.read.csv(\"ml-32m/movies.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4956c025-2090-4463-9eb4-f8557610fce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|     17|   4.0|944249077|\n",
      "|     1|     25|   1.0|944250228|\n",
      "|     1|     29|   2.0|943230976|\n",
      "|     1|     30|   5.0|944249077|\n",
      "|     1|     32|   5.0|943228858|\n",
      "|     1|     34|   2.0|943228491|\n",
      "|     1|     36|   1.0|944249008|\n",
      "|     1|     80|   5.0|944248943|\n",
      "|     1|    110|   3.0|943231119|\n",
      "|     1|    111|   5.0|944249008|\n",
      "+------+-------+------+---------+\n",
      "only showing top 10 rows\n",
      "+-------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
      "|      5|Father of the Bri...|              Comedy|\n",
      "|      6|         Heat (1995)|Action|Crime|Thri...|\n",
      "|      7|      Sabrina (1995)|      Comedy|Romance|\n",
      "|      8| Tom and Huck (1995)|  Adventure|Children|\n",
      "|      9| Sudden Death (1995)|              Action|\n",
      "|     10|    GoldenEye (1995)|Action|Adventure|...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "ratings.show(10)\n",
    "movies.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ade4ddb4-2d2d-45f3-9296-a3be301d3b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "ratings = ratings.dropna().dropDuplicates()\n",
    "movies = movies.dropna().dropDuplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "660cd954-df7c-4c6c-8b88-99fe242bd3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------+--------------------+------------------+\n",
      "|movieId|        avg_rating|count_rating|               title|            genres|\n",
      "+-------+------------------+------------+--------------------+------------------+\n",
      "| 171011|4.4468302658486705|        1956|Planet Earth II (...|       Documentary|\n",
      "| 159817| 4.444369063772049|        2948| Planet Earth (2006)|       Documentary|\n",
      "| 170705| 4.426538598363572|        2811|Band of Brothers ...|  Action|Drama|War|\n",
      "|    318| 4.404613860039444|      102929|Shawshank Redempt...|       Crime|Drama|\n",
      "| 171495| 4.330081300813008|         615|              Cosmos|(no genres listed)|\n",
      "|    858| 4.317030403371463|       66440|Godfather, The (1...|       Crime|Drama|\n",
      "| 202439| 4.312253641816624|       11670|     Parasite (2019)|      Comedy|Drama|\n",
      "| 179135| 4.300085984522786|        1163|Blue Planet II (2...|       Documentary|\n",
      "| 198185| 4.298684210526316|        1140|   Twin Peaks (1989)|     Drama|Mystery|\n",
      "| 220528|  4.28619153674833|         449|Twelve Angry Men ...|             Drama|\n",
      "+-------+------------------+------------+--------------------+------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, count\n",
    "\n",
    "top_movies = ratings.groupBy(\"movieId\") \\\n",
    "    .agg(avg(\"rating\").alias(\"avg_rating\"), count(\"rating\").alias(\"count_rating\")) \\\n",
    "    .filter(\"count_rating >= 50\") \\\n",
    "    .join(movies, \"movieId\") \\\n",
    "    .orderBy(\"avg_rating\", ascending=False)\n",
    "\n",
    "top_movies.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f733aae5-85af-4183-a753-9660377540cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:======================================>                   (2 + 1) / 3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|             genre|count|\n",
      "+------------------+-----+\n",
      "|             Drama|34175|\n",
      "|            Comedy|23123|\n",
      "|          Thriller|11823|\n",
      "|           Romance|10369|\n",
      "|            Action| 9668|\n",
      "|       Documentary| 9363|\n",
      "|            Horror| 8654|\n",
      "|(no genres listed)| 7080|\n",
      "|             Crime| 6976|\n",
      "|         Adventure| 5402|\n",
      "|            Sci-Fi| 4907|\n",
      "|         Animation| 4617|\n",
      "|          Children| 4520|\n",
      "|           Mystery| 4013|\n",
      "|           Fantasy| 3851|\n",
      "|               War| 2325|\n",
      "|           Western| 1696|\n",
      "|           Musical| 1059|\n",
      "|         Film-Noir|  353|\n",
      "|              IMAX|  195|\n",
      "+------------------+-----+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, split\n",
    "\n",
    "genre_df = movies.withColumn(\"genre\", explode(split(col(\"genres\"), \"\\\\|\")))\n",
    "genre_counts = genre_df.groupBy(\"genre\").count().orderBy(\"count\", ascending=False)\n",
    "genre_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da9f0ca7-5233-49f8-90f9-b61f001bb996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 163:=====================================================> (32 + 1) / 33]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.7991872523969626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import rand\n",
    "\n",
    "# Séparer en train/test\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2])\n",
    "\n",
    "als = ALS(\n",
    "    maxIter=10,\n",
    "    regParam=0.1,\n",
    "    rank=10,\n",
    "    userCol=\"userId\",\n",
    "    itemCol=\"movieId\",\n",
    "    ratingCol=\"rating\",\n",
    "    coldStartStrategy=\"drop\"\n",
    ")\n",
    "\n",
    "model = als.fit(training)\n",
    "\n",
    "predictions = model.transform(test)\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"RMSE =\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7117235b-12da-4677-9d48-d726d1807211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "\n",
    "# Remplacer les \"|\" par des espaces dans la colonne \"genres\"\n",
    "movie_genres = movies.withColumn(\"genres_text\", regexp_replace(col(\"genres\"), r\"\\|\", \" \"))\n",
    "\n",
    "# Tokenisation du texte des genres\n",
    "tokenizer = Tokenizer(inputCol=\"genres_text\", outputCol=\"words\")\n",
    "words_data = tokenizer.transform(movie_genres)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017e4301-fefe-4a42-9da1-6210907836b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 167:>                                                        (0 + 3) / 3]"
     ]
    }
   ],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=100)\n",
    "featurized = hashingTF.transform(words_data)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idf_model = idf.fit(featurized)\n",
    "tfidf_movies = idf_model.transform(featurized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "366ed3fd-91b7-4582-a6cc-565cb5c3c86e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tokenizer, HashingTF, IDF\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m movie_genres = movies.withColumn(\u001b[33m\"\u001b[39m\u001b[33mgenres_text\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgenres\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m|\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m      5\u001b[39m tokenizer = Tokenizer(inputCol=\u001b[33m\"\u001b[39m\u001b[33mgenres_text\u001b[39m\u001b[33m\"\u001b[39m, outputCol=\u001b[33m\"\u001b[39m\u001b[33mwords\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m words_data = tokenizer.transform(movie_genres)\n",
      "\u001b[31mTypeError\u001b[39m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "\n",
    "movie_genres = movies.withColumn(\"genres_text\", col(\"genres\").replace(\"|\", \" \"))\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"genres_text\", outputCol=\"words\")\n",
    "words_data = tokenizer.transform(movie_genres)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=100)\n",
    "featurized = hashingTF.transform(words_data)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idf_model = idf.fit(featurized)\n",
    "tfidf_movies = idf_model.transform(featurized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c293e54-8e2f-4b1b-8ca6-4fc9e5edbdcb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tokenizer, HashingTF, IDF\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m movie_genres = movies.withColumn(\u001b[33m\"\u001b[39m\u001b[33mgenres_text\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgenres\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m|\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m      5\u001b[39m tokenizer = Tokenizer(inputCol=\u001b[33m\"\u001b[39m\u001b[33mgenres_text\u001b[39m\u001b[33m\"\u001b[39m, outputCol=\u001b[33m\"\u001b[39m\u001b[33mwords\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m words_data = tokenizer.transform(movie_genres)\n",
      "\u001b[31mTypeError\u001b[39m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "\n",
    "movie_genres = movies.withColumn(\"genres_text\", col(\"genres\").replace(\"|\", \" \"))\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"genres_text\", outputCol=\"words\")\n",
    "words_data = tokenizer.transform(movie_genres)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=100)\n",
    "featurized = hashingTF.transform(words_data)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idf_model = idf.fit(featurized)\n",
    "tfidf_movies = idf_model.transform(featurized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10303ff5-abe5-421f-b922-0aa4a51fbe6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommandations pour l'utilisateur 1 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------------------------------------------------------------------------------------+\n",
      "|userId|recommendations                                                                                      |\n",
      "+------+-----------------------------------------------------------------------------------------------------+\n",
      "|1     |[{217747, 5.2593064}, {194434, 5.180606}, {289897, 5.154702}, {227066, 5.103722}, {89403, 5.0937047}]|\n",
      "+------+-----------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Recommandations pour l'utilisateur 42 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------------------------------------------------------------------------------------------+\n",
      "|userId|recommendations                                                                                        |\n",
      "+------+-------------------------------------------------------------------------------------------------------+\n",
      "|42    |[{205453, 5.9349294}, {194278, 5.7084913}, {171849, 5.5113006}, {217747, 5.480564}, {265908, 5.479037}]|\n",
      "+------+-------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Recommandations pour l'utilisateur 100 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------------------------------------------------------------------------------------+\n",
      "|userId|recommendations                                                                                       |\n",
      "+------+------------------------------------------------------------------------------------------------------+\n",
      "|100   |[{194278, 5.2350717}, {289897, 5.203506}, {205453, 5.143176}, {275847, 5.1248984}, {194434, 5.069031}]|\n",
      "+------+------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Recommandations pour l'utilisateur 120 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------------------------------------------------------------------------------------------+\n",
      "|userId|recommendations                                                                                        |\n",
      "+------+-------------------------------------------------------------------------------------------------------+\n",
      "|120   |[{194434, 6.4531164}, {227066, 6.216584}, {289897, 6.0695543}, {275847, 5.897304}, {193817, 5.8357997}]|\n",
      "+------+-------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Recommandations pour l'utilisateur 150 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 812:=========================>                               (4 + 5) / 9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------------------------------------------------------------------------------------+\n",
      "|userId|recommendations                                                                                       |\n",
      "+------+------------------------------------------------------------------------------------------------------+\n",
      "|150   |[{193817, 6.01458}, {194434, 5.9709315}, {289897, 5.800478}, {227066, 5.7791195}, {282453, 5.4832706}]|\n",
      "+------+------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "user_ids = [1, 42, 100, 120, 150]\n",
    "\n",
    "for user_id in user_ids:\n",
    "    print(f\"Recommandations pour l'utilisateur {user_id} :\")\n",
    "    user_recs = model.recommendForAllUsers(5).filter(col(\"userId\") == user_id)\n",
    "    user_recs.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5c9479-9ff0-4834-9c80-1f41424d0672",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
